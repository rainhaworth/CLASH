# Conserved-Blocks
A deep learning pipline for unsupervised identification of shared segments of DNA across a microbial population.

## Keras Implementation
Two versions of this implementation are available: a standard "full-attention" transformer and one using Big Bird's sparse attention mechanism. The latter is currently in development and is desirable for its ability to handle longer sequences.

Both make use of `tfv2transformer/input.py`, which contains methods to convert DNA sequences into (tokenized) 8-mer sequences that can be handled by the transformer.

### Sparse Attention
The transformer model is defined in `tfv2transformer/transformer_sparse.py`.

The sparse attention mechanism is defined in `tfv2transformer/attention.py`, which was taken from the Big Bird repo. 

The model can be trained by running `train-tfv2-sparse.py`, which will save the model weights to a user-defined file. 

Once a file containing model weights has been generated, it can be used with `cluster-tfv2-dbscan.py` to generate clusters. Additionally, the `dist-test.py` file can be used to investigate the effects of any distance metric. 

### Full Attention
The transformer model is defined in `tfv2transformer/transformer.py`.

The model can be trained by running `train-tfv2.py`, which will save the model weights to a user-defined file.

With minimal modification, the `cluster-tfv2-dbscan.py` and `dist-test.py` scripts can also be used with the full attention implementation.

## FFT Implementation
Not currently in development. Encodings are implemented in `fft_utils.py`. Dataset directory and output directory must currently be manually set by editing the `.py` file they appear in. Other parameters are possible to set with command line arguments unless otherwise stated. 

### Clustering
Dataset and output directory required.

Using hardcoded parameters: `python fft_cluster_density.py`

Using command line: `python fft_cluster_density.py minPts epsilon method`

Valid methods: `ML-DSP` (Purine-Pyrimidine), `ML-DSP-INT`, `ML-DSP-REAL`, `MAFFT_COMPRESSED`

### Conserved Block Extraction
Dataset and output directory required.

Using hardcoded parameters: `python fft_block_density.py`

Using command line: `python fft_block_density.py labelfile`

`labelfile` will be automatically generated by `fft_cluster_density.py` and stored in your output directory. It will not be named `labels.pickle`, so it will be necessary to either update the hardcoded parameter or use the command line argument.

### BLAST Support
Dataset and output directory required. Additionally, you must run `blastn -outfmt 0 -subject /path/to/data.fasta -query /path/to/data.fasta > blast-out.txt` for your dataset then set the `blastfile` variable in `fft_cluster_eval.py` to the full file string for the output alignment data.

Using hardcoded parameters: `python fft_cluster_eval.py`

Using command line: `python fft_cluster_eval.py labelfile`

### Nucleotide Support Plots
Output directory required. Uses generated conserved block support data from most recent `fft_block_density.py` run, stored as `block_cluster_0.pickle`, `block_cluster_1.pickle`, etc. Delete data from previous runs; e.g., if one run generates 10 clusters, then the next generates 5, the last 5 sets of block support values in the output plot will be from the previous run unless they have been deleted.

`python plot.py`

## Transformer Implementations
[Big Bird](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html)

[Keras/TFv2 Transformer](https://github.com/lsdefine/attention-is-all-you-need-keras)

## Pretrained Embedding Model
[dna2vec](https://arxiv.org/abs/1701.06279)

Currently only used for the list of all 8-mers stored in `utils/8mers.txt`.
